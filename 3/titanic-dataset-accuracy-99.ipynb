{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/titanic/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=pd.read_csv('../input/titanic/test.csv')\ny_test=pd.read_csv('../input/titanic/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have already read the data and now its time for data preprocessing.Looking at the various mean values and also finding if there is any null value in our dataset and various other steps .\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can see that there are null values in our Cabin and Age column but a better way to see this is by plotting heatmap using seaborn lib or by just getting info about the datset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can see that there are null values in **Embarked column** as well and we might want to remove this null values for a better result.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So lets deal with the Age dataset first and try to replace these NaN values with some values that can do well.One way to do so is to just replace the values with the mean of the ages of all but that will not be a good option as you will see further how the value changes. \n\nLets plot some of the graphs and identify the relationship of age with other features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot('Pclass','Age',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we can see that the average age for Pclass 1 is around 38 where as for other classes its 29 and 23 respectively. So just by doing the mean we would have replaced the value to 29.6 which would have not performed as good as this . So lets replace the NaN values with the values we found for each class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Age_1(cols) :\n    Age=cols[0]\n    Pclass=cols[1] \n    \n    if pd.isnull(Age) :\n        if Pclass==1 :\n            return 38\n        elif Pclass==2:\n            return 29\n        else :\n            return 24\n    else :\n        return Age\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age']=train[['Age','Pclass']].apply(Age_1,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have replaced the values of Age column with their respective mean values.\n\nNow by seeing the data we can see that Passengerid, Cabin , Ticket, and Name is of no use for determining whether he survived or not .So lets just remove them from our train as well as test dataset.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['PassengerId','Ticket','Cabin','Name'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop(['PassengerId','Ticket','Cabin','Name'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now there is NaN value in Embarked column and that also only two of them . So Just dropping these two values wont make much difference in our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.dropna(axis=0,how='any')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see if our dataset contains any NaN values or not . ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will also have to remove the NaN values and replace it will other values and repeat the same steps on your X_test dataset also bcz it also contains null values. I have just done that in one cell but you can do it by following previous steps . ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot('Pclass','Age',data=X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Age_1(cols) :\n    Age=cols[0]\n    Pclass=cols[1] \n    \n    if pd.isnull(Age) :\n        if Pclass==1 :\n            return 42 \n        elif Pclass==2:\n            return 25\n        else :\n            return 22\n    else :\n        return Age\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['Age']=X_test[['Age','Pclass']].apply(Age_1,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Lets start visualising it in depth and get some insight out of it .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Survived',hue='Sex',data=train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This clearly shows that a lot of females survived over males .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot('Pclass','Fare',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This tells us that for Pclass 1 they actually paid higher money while other classes didnt pay much money but also Pclass 1 and 3 have a lotmore outliers that would led to overfitting so it must be taken care off.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Survived',hue='Pclass',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Survival rate of Pclass is much more than other but for Pclass 2 is almost the same and so by seeing this we cant just ignore the Fare feature and drop it and it is very crucial for further predictions. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(train['Fare'],bins=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This doesnt look like a proper gaussian curve and might tend to overfit the curve as it has very less values of people survived for fare greater than 80$.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fare = train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\nplt.figure(figsize=(15,5))\nsns.distplot(train_fare,bins=40)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see that the curve it equally distributed and will perfrom well under any classifier.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fare=train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing the Fare values by their logerithmic values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets replace the Embarked and Sex columns with their binary values\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sex=pd.get_dummies(train['Sex'],drop_first=True)\nemb=pd.get_dummies(train['Embarked'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train=train['Survived']\ntrain.drop(['Embarked','Sex','Survived','Fare'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dummies are created and now the original dataset we need to concate with the newly created one ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=pd.concat([train,sex,emb,fare],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.drop('PassengerId',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our final dataset for training the dataset and same goes for y_train .\n\nI have to perform the following steps on X_test i.e. test data as well bcz even it contains some of the data like this \n.Repeat the steps\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_fare=X_test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sex=pd.get_dummies(X_test['Sex'],drop_first=True)\ntest_emb=pd.get_dummies(X_test['Embarked'],drop_first=True)\nX_test.drop(['Embarked','Sex','Fare'],inplace=True,axis=1)\nX_test=pd.concat([X_test,test_sex,test_emb,test_fare],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test=pd.concat([X_test,test_sex,test_emb,test_fare,y_test],axis=1)\nY_test=Y_test.dropna(axis=0,how='any')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test=Y_test['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we only problm we would face is that our X_test contains 417 rows of data whereas our y_test contains 418 rows of data . So instead initiall while removing the NaN value from fare dataset we would have just used OneHotEncoder for categorical data and IterativeImputer for Numerical data to solve it . But it just one value other way is to just concate it and then drop that value ,  so that it will remove it from our y_test dataset as well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now our dataset preprocessing and visualisation is complete and now its time to use the proper method to train and test the dataset to get the best output.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Using Logistic Regression Algorithm ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgr=LogisticRegression(max_iter=500)\nlgr_train=lgr.fit(X_train,y_train)\ny_pred = lgr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf=RandomForestClassifier(n_estimators=1000,max_leaf_nodes=16,n_jobs=-1)\nrnd_clf.fit(X_train,y_train)\ny_pred=rnd_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Bagging technique \n\nAlso used GridSearch to use proper hyperparameter and results are much better than using just the logistic regression algorithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = [{'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}]\nb_clf=BaggingClassifier(GridSearchCV(DecisionTreeClassifier(random_state=42),params,cv=3,verbose=1),n_estimators=1000,max_samples=100,bootstrap=True,n_jobs=-1)\nb_clf.fit(X_train,y_train)\ny_pred=b_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_submit=pd.DataFrame(y_pred)\ny_submit.to_csv(\"Submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So for the last technique i.e. bagging I almost got 99.5% accuracy.Though we can use various other techniques such as voting Classifier combining various types of supervised learning algorithm and achieve accuracy may be better or almost equal to this.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Plz feel free to inform if any mistakes are there in this model . If you liked this model plz give a upvote.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Thank you !","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}