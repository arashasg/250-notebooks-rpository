{"cells":[{"metadata":{"_uuid":"2033cb9b5673b58308b984f6f886c5d0bbfef7c7"},"cell_type":"markdown","source":"Ever wondered what a convolution actually looks like? Here is a simple way to visualize the outputs of a convolutional layer."},{"metadata":{"_uuid":"1300c93bdbe9a7f36fbfd4d6e911094cd7ed1d8a"},"cell_type":"markdown","source":"**Setup and Import**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ntest = pd.read_csv('../input/test.csv')\ntest = StandardScaler().fit_transform(np.float32(test.values))\ntest = test.reshape(-1, 28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad3f70f0727b3276600e153ba4690113de4f6518"},"cell_type":"markdown","source":"We can simply use the test set, since there is no need for labels. We only need to reshape it into 28x28 pictures. The StandardScaler() makes the pictures look prettier."},{"metadata":{"_uuid":"9e5d341e270af3cb892e4d3659843cebd57a6f2c"},"cell_type":"markdown","source":"**A first look**\n\nLet's look at a single convolutional layer with a max-pooling operation. We can start by choosing any picture k from the test set. The parameters below allow us to choose what actually happens in our convolutional layer, e.g. how large the kernel is and how wide the strides are.  We can change them later in order to see how they affect the result."},{"metadata":{"trusted":true,"_uuid":"f911e8fe8051fd0923d50040aa297dc3ff586032"},"cell_type":"code","source":"k = 7 #choose which image to take from the test set\nkernel = 4 #Kernel size\nstrides = 1 #Stride size\nkernel_mp = 2 #Kernel size of the max pooling operation\nstrides_mp = 1 #Stride size in max pooling operation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ce0f8afacf7abd707217d648174496065ef544b"},"cell_type":"markdown","source":"Note that changing the strides is what will reduce the dimensionality of our image. Changing the kernel size results in a more blurry image.\n\nBefore we start, let's check what picture we have chosen. We'll use imshow() for that."},{"metadata":{"trusted":true,"_uuid":"9e1f57a2cdbde251751afabb927e68ab30cb6614"},"cell_type":"code","source":"plt.imshow(test[k,:,:,0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ee6661afb87b6e4a5ce87d77d9996cf6fc31ee8"},"cell_type":"markdown","source":"Okay, now let's implement a simple convolution depending on the parameters we have chosen earlier. Let x be the input, w the weight matrices, cl the convolutional layer and cl_mp the max pooling operation applied to the convolutional layer."},{"metadata":{"trusted":true,"_uuid":"e5c0e6a5f8ddaec46f3abcec4ddd265e0a62bd0c"},"cell_type":"code","source":"graph = tf.Graph()\nwith graph.as_default():\n    x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n    w = tf.Variable(tf.truncated_normal([kernel, kernel, 1, 16], stddev=0.1))\n    cl = tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding='SAME')\n    cl_mp = tf.nn.max_pool(cl, ksize=[1, kernel_mp, kernel_mp, 1], strides=[1, strides_mp, strides_mp, 1], padding='SAME')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ea85626f23c16d3c7a20bfca4f07f978632c056"},"cell_type":"markdown","source":"We can now visualize this data. We chose a depth (number of outputs) of 16, so we end up with 16 different images. Let's display them in a grid. The size of the images depends on the stride parameters."},{"metadata":{"trusted":true,"_uuid":"81defee45d7c1a7877cbcd63b886b7236648f598"},"cell_type":"code","source":"with tf.Session(graph=graph) as sess:\n    tf.global_variables_initializer().run()\n    CL = sess.run(cl, feed_dict={x:[test[k]]})\n    print(\"Image Shape =\", CL.shape[1:3])\n    f, ax = plt.subplots(4,4, figsize=(20,20))\n    for i in range(16):\n        ax[i//4,i%4].imshow(CL[0,:,:,i])\n        ax[i//4,i%4].axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc277d04f7c9a15bf5b539ddec6f6900c9ab2533"},"cell_type":"markdown","source":"And after the max pooling?"},{"metadata":{"trusted":true,"_uuid":"9b791b589d82c8b9b4c8ae926b46b27f4f693ac8"},"cell_type":"code","source":"with tf.Session(graph=graph) as sess: #start the session\n    tf.global_variables_initializer().run() #initialize variables\n    CL = sess.run(cl_mp, feed_dict={x:[test[k]]})\n    print(\"Image Shape =\", CL.shape[1:3])\n    f, ax = plt.subplots(4,4, figsize=(20,20))\n    for i in range(16):\n        ax[i//4,i%4].imshow(CL[0,:,:,i])\n        ax[i//4,i%4].axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f610c328f66a205d527e9d0ed3139028f7a0c78f"},"cell_type":"markdown","source":"**Multilayer Network**\n\nSo now let's take a look how our features change when we look at multiple layers, each with its own max pooling operation. We use 3 layers to give us an idea of the downsampling happening with each step. By choice of the stride of the max-pooling, the image size is reduced by half at each step, so we end up with 4x4 pictures at the end of layer 3. Also by choice of the depth (number of outputs) we'll have exactly 64 of those."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"graph = tf.Graph()\nwith graph.as_default():\n\n    x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n    w1 = tf.Variable(tf.truncated_normal([8, 8, 1, 16], stddev=0.1))\n    w2 = tf.Variable(tf.truncated_normal([5, 5, 16, 32], stddev=0.1))\n    w3 = tf.Variable(tf.truncated_normal([3, 3, 32, 64], stddev=0.1))\n    \n    # Convolutional layer 1\n    cl1 = tf.nn.conv2d(x, w1, strides=[1, 1, 1, 1], padding='SAME')\n    cl1_mp = tf.nn.max_pool(cl1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')        \n    # Convolutional layer 2\n    cl2 = tf.nn.conv2d(cl1_mp, w2, strides=[1, 1, 1, 1], padding='SAME')\n    cl2_mp = tf.nn.max_pool(cl2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n    # Convolutional layer 3\n    cl3 = tf.nn.conv2d(cl2_mp, w3, strides=[1, 1, 1, 1], padding='SAME')\n    cl3_mp = tf.nn.max_pool(cl3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d13bdfe5bf1866ccdc6dc115d2d4a6833ab31b"},"cell_type":"markdown","source":"Now take a look at the downsampling operations of a multilayer convolutional network."},{"metadata":{"trusted":true,"_uuid":"388831f5ec8ac2e4a2a6b9271570043476b544f0"},"cell_type":"code","source":"#After Layer 1\nwith tf.Session(graph=graph) as sess: #start the session\n    tf.global_variables_initializer().run() #initialize variables\n    CL = sess.run(cl1_mp, feed_dict={x:[test[k]]})\n    print(\"Image Shape =\", CL.shape[1:3])\n    print(\"Number of Images =\", CL.shape[-1])\n    f, ax = plt.subplots(4,4, figsize=(20,20))\n    for i in range(16):\n        ax[i//4,i%4].imshow(CL[0,:,:,i])\n        ax[i//4,i%4].axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f53286b6c75c777c75bd55c45dea7a4f4953deba"},"cell_type":"code","source":"#After Layer 2\nwith tf.Session(graph=graph) as sess: #start the session\n    tf.global_variables_initializer().run() #initialize variables\n    CL = sess.run(cl2_mp, feed_dict={x:[test[k]]})\n    print(\"Image Shape =\", CL.shape[1:3])\n    print(\"Number of Images =\", CL.shape[-1])\n    f, ax = plt.subplots(4,8, figsize=(20,10))\n    for i in range(32):\n        ax[i//8,i%8].imshow(CL[0,:,:,i])\n        ax[i//8,i%8].axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e23ae38f35b8e6cf6ffc7a89ce3d6194bb5c3d03"},"cell_type":"code","source":"#After Layer 3\nwith tf.Session(graph=graph) as sess:\n    tf.global_variables_initializer().run()\n    CL = sess.run(cl3_mp, feed_dict={x:[test[k]]})\n    print(\"Image Shape =\", CL.shape[1:3])\n    print(\"Number of Images =\", CL.shape[-1])\n    f, ax = plt.subplots(8,8, figsize=(20,20))\n    for i in range(64):\n        ax[i//8,i%8].imshow(CL[0,:,:,i])\n        ax[i//8,i%8].axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4302aa7c805fa6bb5972ae9b856aa3ae244dca2"},"cell_type":"markdown","source":"Looks cool, right? The images above are what will be fed into the fully connected layer in order to be classified."},{"metadata":{"_uuid":"dfae19c28b2dcfc6bde45d9b9eb505c8a641e6af"},"cell_type":"markdown","source":"**Great Resources**\n\nhttps://www.youtube.com/watch?v=FG7M9tWH2nQ\n\nhttps://www.youtube.com/watch?v=BFdMrDOx_CM"},{"metadata":{"trusted":true,"_uuid":"35a2c286888998b34584d9ab316327b1ab4077e7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}