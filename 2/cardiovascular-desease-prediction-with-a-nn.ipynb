{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Classifikation of a cardiovascular desease with a Neuronal Network\nThis is my first kernel here on kaggle. \nIn this kernel I only drop some data with medically impossible value. There is few data analysis or visualisation, because there are some great examples in other kernels. Furthermore I am not experienced in data engeneering  (maybe you can give me some hints ;) and some things i tried beforehand had no positive impact towards the accuracy of my network."},{"metadata":{"_uuid":"8da24b6e-12b2-4de1-be60-90cccf50d9fa","_cell_guid":"dfbdf190-e7a3-4a9a-9681-c0c773b996be","trusted":true},"cell_type":"code","source":"# Imports\nimport numpy as np \nimport pandas as pd \nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.optimizers import *\nfrom keras.initializers import *\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n\n# Get data\ndf = pd.read_csv('/kaggle/input/cardiovascular-disease-dataset/cardio_train.csv', delimiter=';')\ndf.drop('id', axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the min and max values of 'ap_hi' and 'ap_lo' are medically not possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['ap_lo'] >= df['ap_hi']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Higher diastolic than systolic blood pressure is impossible, too.\nSo let's remove these."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[\"ap_lo\"] > df[\"ap_hi\"]].index, inplace=True)\ndf.drop(df[df[\"ap_lo\"] <= 30].index, inplace=True)\ndf.drop(df[df[\"ap_hi\"] <= 40].index, inplace=True)\ndf.drop(df[df[\"ap_lo\"] >= 200].index, inplace=True)\ndf.drop(df[df[\"ap_hi\"] >= 250].index, inplace=True)\ndf[['ap_lo', 'ap_hi']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is time to get our X and Y, to split the data in train and test sets and to scale it."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('cardio', axis=1)\nY = df['cardio']\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\ns = StandardScaler()\nx_train = s.fit_transform(x_train)\nx_test = s.transform(x_test)\n\n# Split train set in train and validation set:\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the NN is the next step."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Silence warnings\nimport warnings as w\nw.simplefilter('ignore')\n\n\ndef create_model():\n    # Hyperparameter:\n    init_w = glorot_uniform(seed=0)\n    loss = \"binary_crossentropy\"\n    optimizer = Adadelta()\n    \n    # Defining the model:\n    model = Sequential()\n\n    model.add(Dense(50, kernel_initializer=init_w, input_shape=(x_train.shape[1],)))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    model.add(Dropout(rate=0.1))\n\n    model.add(Dense(25, kernel_initializer=init_w))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    model.add(Dropout(rate=0.1))\n\n    model.add(Dense(12, kernel_initializer=init_w))\n    model.add(LeakyReLU())\n\n    model.add(Dense(1, kernel_initializer=init_w))\n    model.add(Activation(\"sigmoid\"))\n    \n    model.summary()\n    \n    # Training\n    model.compile(\n        loss=loss,\n        optimizer=optimizer,\n        metrics=[\"accuracy\"])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn = create_model()\nnn.fit(\n    x=x_train,\n    y=y_train,\n    verbose=2,\n    epochs=50,\n    batch_size=256,\n    validation_data=[x_valid, y_valid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing\ntest_score = nn.evaluate(x_test, y_test)\nprint(\"Testing Acc:\", test_score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get an accuricy on the test set between: **73.3% - 73.7%**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = nn.predict(x_test)\ncm = confusion_matrix(y_test, y_pred.round())\nprint(\"Confusion Matrix:\", \"\\n\", cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpr, fpr, threshold = roc_curve(y_test, y_pred)\nauc_score = roc_auc_score(y_test, y_pred)\nprint(\"AUC-score:\", auc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.plot(tpr, fpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To me this looks pretty ok.\nHints and advice would be welcome. (I evaluated the hyperparameter that worked best for me beforehand with some runs of a GridSearch. This is not in the kernel, because of the required runtime. If you want me to share my code for the GridSearch or KFold, feel free to tell me. I am going to put it online then.)"},{"metadata":{},"cell_type":"markdown","source":"Thank you for visiting my very first Kernel :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}