{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple Example Using LIME."},{"metadata":{},"cell_type":"markdown","source":"LIME = (L)ocal (I)nterpretable (M)odel-agnostic (E)xplanations)"},{"metadata":{},"cell_type":"markdown","source":"## Overview"},{"metadata":{},"cell_type":"markdown","source":"- About example program.\n - Visualize the effectiveness of each explanatory variable related to the survival probability of Titanic by using LIME.\n - Created for the purpose of understanding LIME physically.\n - Program created as binary classification problem of logistic regression. \n - Selecting features and handling-missing-values are decided on my own.\n - Use only Train data.\n- Hot to install LIME.\n - pip install lime\n- Future version upgrade plans.\n - Inplement considering Multiple collinearity.\n    - However, \"Simple and High Quality\" is mandatory.\n    - Anyway, I had tried using get_dummies / LabelBinarizer. but implemented spaghetti code, so I gave up.\n- Reference URL.\n - https://lime-ml.readthedocs.io/en/latest/index.html\n - https://github.com/marcotcr/lime\n - https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5"},{"metadata":{},"cell_type":"markdown","source":"Japanese Translation with jargon style. (Please use at your own risk when making reference. :))\n- サンプルプログラムについて\n - タイタニックの生存確率に関係する各説明変数の効き具合を、LIMEを使って行単位でビジュアル化。\n - LIMEの動きを理屈でなく体感的に感じとることを目的に作成。\n - モデルはロジスティック回帰の二値分類問題。\n - 特徴量の選択とか、欠損値の扱いは適当。\n - Trainデータのみ使用。\n- LIMEのインストール方法\n - pip install lime\n- 今後のバージョンアップ予定\n - 多重共線（マルチコ）を考慮した実装にする。\n    - 但し、ごり押しではなく「綺麗に」実装する。\n    - 一応、get_dummies や LabelBinarizer で試みたが、ごり押しになったのでやめた。\n- 参考サイト\n - https://lime-ml.readthedocs.io/en/latest/index.html\n - https://github.com/marcotcr/lime\n - https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n# LIME\nimport lime.lime_tabular\nfrom lime.explanation import Explanation\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Load Titanic DataSet\ndf = pd.read_csv('../input/train.csv')\ndf.isnull().sum()  # check missing data.","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[],"trusted":false},"cell_type":"code","source":"# Remove Useless Attributes\ndf.drop(['PassengerId', 'Name', 'Pclass', 'SibSp',\n         'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n\n# Handle Missing Data\nage_train_mean = df.groupby('Sex').Age.mean()\ndf.loc[df['Age'].isnull() & (df['Sex'] == 'male'),\n       'Age'] = age_train_mean['male']\ndf.loc[df['Age'].isnull() & (df['Sex'] == 'female'),\n       'Age'] = age_train_mean['female']\n\ndf.dropna(subset=['Embarked'], axis=0, inplace=True)\n\nprint(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing pre-encoded dataframe"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.catplot(data=df, kind='violin', hue='Survived',\n            x='Embarked', y='Age', col='Sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = df.drop(['Survived'],  axis=1, inplace=False)\ny_train = df.Survived","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding"},{"metadata":{},"cell_type":"markdown","source":"- Note\n - LimeTabularExplainer handles Label Encoded Data. So, it's  difficult to select get_dummies or LabelBinalizer. "},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_lbenc = X_train.copy()\ncats = ['Sex','Embarked'] # not yet specified label encoded attributes.\n\ncat_dic = {}  # also be used at LimeTabularExplainer's parameter.\ncat_list = [] # also be used at OneHotEncoder, LimeTabularExplainer's parameter.\n\nle = LabelEncoder()\nfor s in cats:\n    i = X_train_lbenc.columns.get_loc(s)\n    X_train_lbenc.loc[:,s] = le.fit_transform(X_train_lbenc[s])\n    cat_dic[i] = le.classes_\n    cat_list.append(i)\n\nX_train_lbenc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(cat_list, '\\n',  cat_dic) # check","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Non-categorical features are always stacked to the right of the matrix.\noe = OneHotEncoder(sparse=False, categorical_features=cat_list)\noe_fit = oe.fit(X_train_lbenc)\nX_train_ohenc = oe_fit.transform(X_train_lbenc)\nX_train_ohenc[:5, :]  # show 5 samples.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"parameters = {\n    'C': np.logspace(-5, 5, 10),\n    'random_state': [0]\n}\n\ngs = GridSearchCV(\n    LogisticRegression(),\n    parameters,\n    cv=5\n)\ngs.fit(X_train_ohenc, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)\n\nmodel = LogisticRegression(**gs.best_params_)\nmodel.fit(X_train_ohenc, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing explanations for each data"},{"metadata":{"trusted":false},"cell_type":"code","source":"explainer = lime.lime_tabular.LimeTabularExplainer(X_train_lbenc.values,  # Label Encoded Numpy Format\n                                                   feature_names = X_train_lbenc.columns,\n                                                   class_names = [\n                                                       'dead', 'survive' ], # 0,1,...\n                                                   categorical_features = cat_list,\n                                                   categorical_names = cat_dic,\n                                                   mode = 'classification'\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def pred_fn(x):\n    return model.predict_proba(oe_fit.transform(x)).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp = explainer.explain_instance(X_train_lbenc.values[2, :],\n                                 pred_fn,\n                                 num_features=len(X_train_lbenc.columns)\n                                 )\nexp.show_in_notebook(show_all=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}