{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import libraries\n\nWe are going to use TPUs for this, since transformers including distilBERT are very heavy duty, and we'll need a lot of computational power for this one.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\n\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding, LSTM, Conv2D, Conv1D, MaxPooling1D, Dense, Dropout, GlobalMaxPooling1D, Input, Bidirectional, concatenate, Flatten, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\n# physical_devices = tf.config.list_physical_devices('GPU')\n# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TRAIN_FILE_PATH = '/kaggle/input/ag-news-classification-dataset/train.csv'\nTEST_FILE_PATH = '/kaggle/input/ag-news-classification-dataset/test.csv'\n\n\ndata = pd.read_csv(TRAIN_FILE_PATH)\ntestdata = pd.read_csv(TEST_FILE_PATH)\n\nX_train = data['Title'] + \" \" + data['Description']\ny_train = data['Class Index'].apply(lambda x: x-1).values # Classes need to begin from 0\n\nx_test = testdata['Title'] + \" \" + testdata['Description']\ny_test = testdata['Class Index'].apply(lambda x: x-1).values # Classes need to begin from 0\n\nmaxlen = X_train.map(lambda x: len(x.split())).max()\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define tokenizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 20000\nembed_size = 32\ndistil_bert = 'distilbert-base-uncased'\n\ntokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n                                                max_length=maxlen, pad_to_max_length=True)\n\ndef tokenize(sentences, tokenizer):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, pad_to_max_length=True, \n                                             return_attention_mask=True, return_token_type_ids=True)\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])        \n        \n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenize data using defined tokenizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize desc and title train data\nX_train = tokenize(X_train, tokenizer)\nx_test = tokenize(x_test, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define model in TPU scope","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n    config.output_hidden_states = False\n    transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config=config)\n\n    input_ids_in = tf.keras.layers.Input(shape=(maxlen,), name='input_token', dtype='int32')\n    input_masks_in = tf.keras.layers.Input(shape=(maxlen,), name='masked_token', dtype='int32') \n\n    embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer)\n    X = tf.keras.layers.GlobalMaxPool1D()(X)\n    X = tf.keras.layers.Dense(64, activation='relu')(X)\n    X = tf.keras.layers.Dropout(0.2)(X)\n    X = tf.keras.layers.Dense(4, activation='sigmoid')(X)\n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\n    for layer in model.layers[:3]:\n        layer.trainable = False\n\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compile and fit model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n#     EarlyStopping(\n#         monitor='val_accuracy',\n#         min_delta=1e-4,\n#         patience=4,\n#         verbose=1\n#     ),\n    ModelCheckpoint(\n        filepath='weights.h5',\n        monitor='val_accuracy', \n        mode='max', \n        save_best_only=True,\n        save_weights_only=True,\n        verbose=1\n    )\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, batch_size=1024, validation_data=(x_test, y_test), epochs=20, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test model with some arbitrary data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n\ntest = ['New evidence of virus risks from wildlife trade', 'Coronavirus: Bank pumps £100bn into UK economy to aid recovery', \n        'Trump\\'s bid to end Obama-era immigration policy ruled unlawful', 'David Luiz’s future with Arsenal to be decided this week']\ntest_seq = tokenize(test, tokenizer)\ntest_preds = [labels[np.argmax(i)] for i in model.predict(test_seq)]\n\nfor news, label in zip(test, test_preds):\n    print('{} - {}'.format(news, label))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = [np.argmax(i) for i in model.predict(x_test)]\ncm  = confusion_matrix(y_test, preds)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(4), labels, fontsize=12)\nplt.yticks(range(4), labels, fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get precision and recall scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Recall of the model is {:.2f}\".format(recall_score(y_test, preds, average='micro')))\nprint(\"Precision of the model is {:.2f}\".format(precision_score(y_test, preds, average='micro')))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}