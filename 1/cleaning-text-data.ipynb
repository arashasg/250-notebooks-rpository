{"cells":[{"metadata":{"_uuid":"722bd76ea02dd15a1206f11841e8ca0d0b00564e"},"cell_type":"markdown","source":"# <font color=\"red\">Cleaning Text Data</font>\nIn this notebook, I mainly cited\n\n[Jason Brownlee's post (How to Clean Text for Machine Learning with Python)](https://machinelearningmastery.com/clean-text-machine-learning-python/)\n\n[Kendall Fortney's post (Pre-Processing in Natural Language Machine Learning)](https://towardsdatascience.com/pre-processing-in-natural-language-machine-learning-898a84b8bd47)\n\n[Maria Dobko's post (Text Data Cleaning and Preprocessing)](https://medium.com/@dobko_m/nlp-text-data-cleaning-and-preprocessing-ea3ffe0406c1)"},{"metadata":{"_uuid":"d3c0406b75aac9ebdbb8d871e03c923f0a4c2b02"},"cell_type":"markdown","source":"## Introduction\n\nTeaching a computer accurately understand word-context has been an unsolved problem for a long time. Words with the same meaning can exist in a variety of expressions, and there are even new words born every day.\n\nTo solve this problem, a variety of research directions are under way which require huge text datasets for their respective purpose.\n\nCleaning text-data is a typical pre-processing task for data science and machine learning.\n\nIt consists of getting rid of the less useful parts of text through stopword removal, dealing with capitalization, special characters and other details.\n\nToday we’re going to do cleaning text  from Kafka’s famous book Metamorphosis, as described in [Jason Brownlee’s post](https://machinelearningmastery.com/clean-text-machine-learning-python/).\n\n[Download]()\n\n[Metamorphosis by Franz Kafka Plain Text UTF-8](http://www.gutenberg.org/ebooks/5200?msg=welcome_stranger).\n\nOpen the file and delete the header and footer information and save the file as “metamorphosis_clean.txt“.\n\nWe are using python3 for the example.\n\n "},{"metadata":{"_uuid":"b9e3ca6b6ce562427d2aba55bc8c3894c5dea3fb"},"cell_type":"markdown","source":"## <font color=\"navy\">NLTK </font>\n### <font color=\"forestGreen\">The Natural Language Toolkit, or NLTK for short, is a Python library written for working and modeling text.</font>\n### <font color=\"forestGreen\">It provides a high-level api to flexibly implement a variety of cleaning methods.</font>"},{"metadata":{"_uuid":"dd07fe2bfef5e090a8bbe39afcdb19cfff370f1f"},"cell_type":"markdown","source":"## Step 1. Sneak peek into the data"},{"metadata":{"_uuid":"a931f658a6d1e9835f333c0452183fef786c0c9c"},"cell_type":"markdown","source":"Take a look at the data: explore its main characteristics like size and structure to see how sentences, paragraphs, text are built.\n\n* Understand how much of this data is useful for your needs.\n* Review the text to see what exactly might help.\n\nIn the case of Metamorphosis:\n\n* There are no obvious typos or spelling mistakes.\n* There’s punctuation like commas, apostrophes, quotes, question marks, and more.\n* Overall it is simple"},{"metadata":{"_uuid":"44ddc86c5ebd0aee44206ec3346ef1e649cf7213"},"cell_type":"markdown","source":"## Step 2. Whitespace/Punctuation/Normalize Case"},{"metadata":{"_uuid":"68423ce75be00e3ad1c2fd2fc6aa56d372f43b32"},"cell_type":"markdown","source":"### 1. Install NLTK\n\n```python\npip install -U nltk\npython -m nltk.downloader all\n```\n* Usage\n```python\nimport nltk\n```"},{"metadata":{"_uuid":"1c05c6dc5115d586035c4dc5d4fbb5c1f6f82828"},"cell_type":"markdown","source":"## 2. Tokenization\n### Split into Words"},{"metadata":{"trusted":true,"_uuid":"6cb7149cc83066341cdafb695aca666cd203956d"},"cell_type":"code","source":"!ls  ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fb74979a8e6bcd59bbe9dac11112f34f0ae8a65"},"cell_type":"code","source":"# load data\nfilename = '../input/metamorphosis_clean.txt'\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()\n# split into words\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(text)\nprint(tokens[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb10b32ec076ecd615b310ddd2fe30e77c3596a"},"cell_type":"markdown","source":"<font color=\"red\">tokens = word_tokenize(text)</font>\n\n<font color=\"black\">It does the same thing as split() we saw above.</font>\nWe can see  <font color=\"red\">'looked', '.', '``', 'What', \"'s\"</font> in this result.\n\nYou may not feel a big difference just by looking at this, but you can easily handle the split by sentences using NLTK. Let’s replace **word_tokenizer** with **sent_tokenizer**"},{"metadata":{"trusted":true,"_uuid":"deae69be363648a2e531b3dbaf3f5be5a12bf1a6"},"cell_type":"code","source":"from nltk import sent_tokenize\nsentences = sent_tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a161a1e83b4e55c6d67b825a97b095e91fa0dcac"},"cell_type":"markdown","source":"Cleaning text is used for a variety of purposes and the flexibility of NTLK allows you to focus on the core rather than on implementation itself."},{"metadata":{"_uuid":"cc026e9ad9d0d31fd6bb0e7087680013caf09bbb"},"cell_type":"markdown","source":"## 3. Filter Out Punctuation"},{"metadata":{"trusted":true,"_uuid":"11ce16d2aae6a86cd332f41e5eec9b5141c546f2"},"cell_type":"code","source":"# load data\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()\n# split into words\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(text)\n# remove all tokens that are not alphabetic\nwords = [word for word in tokens if word.isalpha()]\nprint(words[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06f0c051c953f99f83415859e1265a8613d8a85b"},"cell_type":"markdown","source":"## Step 3. Stopwords/Stemming"},{"metadata":{"_uuid":"6b68af2c88a5d5f5c79ced1cdcf36d91ffff6b10"},"cell_type":"markdown","source":"\nCleaning Text Data\nIn this blog, I mainly cited\nJason Brownlee's post (How to Clean Text for Machine Learning with Python)\nKendall Fortney's post (Pre-Processing in Natural Language Machine Learning)\nMaria Dobko's post (Text Data Cleaning and Preprocessing)"},{"metadata":{"_uuid":"05528f06ea6a357d8e2047e62d66f10b58b528bb"},"cell_type":"markdown","source":"### 1. Filter out Stopwords and Pipelines\nA majority of the words in a given text are connecting parts of a sentence rather than showing subjects, objects or intent. Word like “the” or “and” can be removed by comparing text to a list of stopwords."},{"metadata":{"trusted":true,"_uuid":"7efe842de1cd1e3e93e0c0c4cd89b75d680256d2"},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23b85d5d2f68a7da8be5e218f0ac52074b85455"},"cell_type":"code","source":"# load data\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()\n# split into words\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(text)\n# convert to lower case\ntokens = [w.lower() for w in tokens]\n# remove punctuation from each word\nimport string\ntable = str.maketrans('', '', string.punctuation)\nstripped = [w.translate(table) for w in tokens]\n# remove remaining tokens that are not alphabetic\nwords = [word for word in stripped if word.isalpha()]\n# filter out stop words\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nwords = [w for w in words if not w in stop_words]\nprint(words[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09f15cb11d4c4bee4177a58d8bf9134313d5d2b7"},"cell_type":"markdown","source":"* We can easily get result<br>\n<font color=\"red\">['dreams', 'found', 'transformed', 'bed', 'horrible']</font> from <font color=\"red\">['dreams', 'he', 'found', 'himself', 'transformed']</font> using **<font color=\"red\">set(stopwords.words('english'))</font>**."},{"metadata":{"_uuid":"57d0b3ba976ea321de0078e40ae2cbda292a367e"},"cell_type":"markdown","source":"### 2. Stemming\nStemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix. There are several stemming models, including Porter and Snowball. But there is a danger of “over-stemming” were words like “universe” and “university” are reduced to the same root of “univers”."},{"metadata":{"trusted":true,"_uuid":"a06c9533078620f3c29587cfb95c71e5db8a1ce7"},"cell_type":"code","source":"# load data\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()\n# split into words\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(text)\n# stemming of words\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\nstemmed = [porter.stem(word) for word in tokens]\nprint(stemmed[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b624c8bbe7d1c18c68d92c95d10a26bd87872be"},"cell_type":"markdown","source":"After stemming, <font color=\"red\">['morning', 'troubled']</font> converted to <font color=\"red\">['morn','troubl'] </font>by **<font color=\"red\">PorterStemmer</font>**.\n\nYou can also see that the tokens converted to lowercase.\n\nFinally, we have cleaned text data reducing words to their root by stemming."},{"metadata":{"_uuid":"182a4f64ca931120282b2c1334e1d6f6e9e0f566"},"cell_type":"markdown","source":"## Step 4. Other tools\n### 1. Lemmatization\nLemmatization is also an alternative to removing inflection. By determining the part of text and utilizing WordNet’s lexical database of English, it can get better results.\n\nIt is a more accurate but slower. Stemming may be more useful in queries for databases whereas Lemmatization may work much better when trying to determine text sentiment.\n\n### 2. Word Embedding/Text Vectors\nWord embedding is the modern way of representing words as vectors. The aim of word embeddings is to find a series of high dimensionality vectors (one for each word) that represent the relation of words in such a way that semantically related words are ‘close together’ in that high dimensional space. Word2Vec and GloVe are the most common models for converting text to vectors. Often, T-SNE (as well as PCA) is used to reduce the dimensionality enough to display as a 2 or 3 dimensional graph. Check out this example of T-SNE applied to word embeddings."},{"metadata":{"_uuid":"46f612dd649b6658f3337a22b50cad8fb0a80b6c"},"cell_type":"markdown","source":"## Further Reading\n[Shubham Jain’s post(Ultimate guide to deal with Text Data]\n\nIn that article you can learn different feature extraction methods, starting with some basic techniques which will lead into advanced Natural Language Processing techniques including <code>N-grams, Term Frequency, Inverse Document Frequency, Term Frequency-Inverse Document Frequency (TF-IDF),  Bag of Words and Sentiment Analysis</code>\n\nIn addition, if you want to dive deeper, visit video course on NLP (using Python).\n\n \n## Summary\nIn this notebook, you saw what cleaning text is and looked into it in Python codes.\n\nSpecifically, you learned it by 4 steps\n\nSneak peek into the data\nWhitespace/Punctuation/Normalize Case\nStopwords/Stemming\nOther tools\nAlso, I think you have understood the pros of using NLTK compared to manually implementing it.\n\nCleaning text can be performed variously depending on the what the purpose is. It would be nice to study the methods that I haven’t introduced today at Futher Reading."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}