{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"d496f87559c0dcd19bb0e5048d9faf381425e667","_cell_guid":"60ea2b40-675f-4d5f-9cb3-b52560e7d04d"},"source":"# What is this?\nThis is just a quick test to see how various classifiers compare when trying to predict CDR (clinical dementia rating).\n\n# Comments are very welcome!"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7cd75adb3375f5940d56ede3a2c933828625e16d","_cell_guid":"3b689a8e-078a-4399-a6eb-0234e3a28d3f","collapsed":true},"source":"\n#! -*- encoding: utf-8 -*-\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\nsns.set_style(\"darkgrid\")\n\n% matplotlib inline\n\ndata = pd.read_csv('../input/oasis_longitudinal.csv')\n\n# Fill missing fields\ndata = data.fillna(method='ffill')\n\n# Drop unnecessary columns\nfor x in ['Subject ID', 'MRI ID', 'Visit']:\n    data.drop(x, axis=1, inplace=True)\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"cef33bf8af37d4708a0c3029b262a6581ce42891","_cell_guid":"c71fc119-921b-40c5-9e6e-4c6f981daee5"},"source":"The columns have different types of data in them, but we need integers, so we convert them."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"aec0342b3d90a4db59cbadffa22d4ba93d916713","_cell_guid":"5f9bd169-9228-4b5c-a7ce-3ea931b19dda","collapsed":true},"source":"# Encode columns into integers\nfor column in data.columns:\n    le = LabelEncoder()\n    data[column] = le.fit_transform(data[column])"},{"cell_type":"markdown","metadata":{"_uuid":"8526a70d6dee769211d29fa0f8515e9d84223396","_cell_guid":"c295c6cd-0e36-4673-ace6-940eb23df7d3"},"source":"# Heatmap\nPerhaps the heat map could be used in the future to only use certain columns instead of all."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1cd960bdfa50f6a0aed5fd3cba1410beb10c2001","_cell_guid":"40d7edcc-1167-48f4-b800-5947467b2613","collapsed":true},"source":"# Draw heatmap to show correlation\nsns.heatmap(data.corr(), annot=True)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2a42dee7c3b3f6f8b666a41d2d47a8ab3bac003b","_cell_guid":"d0b13476-674e-4843-ab85-a6ec7e5fd67c","collapsed":true},"source":"# Split training and test data\ntrain, test = train_test_split(data, test_size=0.3)\n\n# Load columns\ntrain_X = train[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']]\ntrain_y = train.CDR\ntest_X = test[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']]\ntest_y = test.CDR\n\n# Pick a few classifiers, source: http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\nclassifiers = [\n    KNeighborsClassifier(2),\n    KNeighborsClassifier(3),\n    KNeighborsClassifier(6),\n    KNeighborsClassifier(7),\n    SVC(),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(kernel=\"linear\", C=0.01),\n    SVC(kernel=\"linear\", C=2),\n    SVC(gamma=2, C=1),\n    # too slow GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    LogisticRegression()]\n\n# Print accuracy\ndata = []\nlabels = []\n\nfor i, model in enumerate(classifiers):\n    model = model\n    model.fit(train_X, train_y)\n    prediction = model.predict(test_X)\n    labels.append(str(model).split('(')[0])\n    data.append([metrics.accuracy_score(prediction, test_y)])\nplt.plot([i for i, e in enumerate(data)], data, 'ro'); plt.xticks([i for i, e in enumerate(labels)], [l[0:3] for l in labels])\n"},{"cell_type":"markdown","metadata":{"_uuid":"f70a369b1378e62e1c41dda0c0e1c56ea34b599d","_cell_guid":"d232483d-68b2-4c88-825a-1a544cd989ab"},"source":"## Result\nIt seems like `SVC(kernel=\"linear\", C=0.01)` had the best results."}],"nbformat_minor":1}