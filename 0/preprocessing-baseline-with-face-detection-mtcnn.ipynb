{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is forked from Marco Vasquez E's wonderful notebook([here](https://www.kaggle.com/marcovasquez/basic-eda-face-detection-split-video-and-roi)). This notebook is a baseline of preprocessing. If you find this helpful, please *upvote* this notebook and the associated dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/mtcnn-package/mtcnn-0.1.0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<a id=\"1\"></a> <br>**\n## 1- Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom time import time\nimport os\nfrom mtcnn import MTCNN\nfrom tqdm import tqdm_notebook\nimport time\nimport gc\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<a id=\"2\"></a> <br>**\n## 2- Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\ntest_dir = '/kaggle/input/deepfake-detection-challenge/test_videos/'\ntest_video_files = [test_dir + x for x in os.listdir(test_dir)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf_train = pd.read_json('/kaggle/input/deepfake-detection-challenge/train_sample_videos/metadata.json').transpose()\ndf_train=dict(df_train)['label']\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABELS = ['FAKE','REAL']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<a id=\"3\"></a> <br>**\n## 3- Define Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"detector = MTCNN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD=0.7\ndef detect_face(img,ratio):\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    final = []\n    detected_faces_raw = detector.detect_faces(img)\n    if detected_faces_raw==[]:\n        print('no faces found')\n        return []\n    for x in detected_faces_raw:\n        if x['confidence']<THRESHOLD:\n            continue\n        x,y,w,h=x['box']\n        #x,y,w,h=ratio*x,ratio*y,ratio*w,ratio*h\n        final.append([x,y,w,h])\n    return final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable  **THRESHOLD**  means the confidence of detected face have to be more than the value or else they will be ignored."},{"metadata":{"trusted":true},"cell_type":"code","source":"RESIZING_RATIO=2\nFACES_TAKE = 1\ndef detect_video(video):\n    face_frames=[]\n    cap = cv2.VideoCapture(video)\n    cap.set(cv2.CAP_PROP_FPS, 24)\n    ret,frame = cap.read()\n    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    t=tqdm_notebook(total=total)\n    while True:\n        t.update()\n        #cap.set(cv2.CAP_PROP_POS_MSEC,(count*1000))   \n        ret,frame = cap.read()\n        if ret!=True:\n            break\n        frame=cv2.resize(frame,(int(1920/RESIZING_RATIO),int(1024/RESIZING_RATIO)))\n        faces = detect_face(frame,RESIZING_RATIO)\n        if faces==[]:\n            face_frames.append([])\n            continue\n        if FACES_TAKE==1:\n            x,y,w,h=faces[0]\n            face_frames.append(frame[y:h+y,x:w+x])\n            continue\n        face_frames.append([])\n        count=0\n        for (x,y,w,h) in faces:\n            count+=1\n            croped_face = frame[y:h+y,x:w+x]\n            face_frames[-1].append(croped_face)\n            if count==FACES_TAKE:\n                break\n    return face_frames","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable  **RESIZING_RATIO**  means the multiplicative inverse of the resizing rate. Making it bigger will make the process faster.\n\nThe variable  **FACE_TAKE**  means how many of the face will be kept."},{"metadata":{"trusted":true},"cell_type":"code","source":"def video(video):\n    cap = cv2.VideoCapture(video)\n    cap.set(cv2.CAP_PROP_FPS, 24)\n    ret,frame = cap.read()\n    final=[]\n    while True:\n        if ret!=True:\n            break\n        ret,frame = cap.read()\n        final.append(frame)\n    return final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_choice(face_frames,num):\n    return [random.choice(face_frames) for _ in range(num)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show(face_frames,num_faces):\n    if FACES_TAKE==1:\n        for x in random_choice(face_frames,num_faces):\n            try:\n                if type(x)!=np.ndarray:\n                    x=np.zeros(face_frames[0].shape)\n                else:\n                    x=cv2.cvtColor(x,cv2.COLOR_BGR2RGB)\n                plt.imshow(x)\n                plt.show()\n            except:\n                pass\n    else:\n        pass #currently working on it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"face_frames=detect_video(train_video_files[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show(face_frames,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<a id=\"4\"></a> <br>**\n## 4- Build pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pipeline(video_files):\n    X_train=[]\n    y_train=[]\n    for video_file in video_files:\n        X=detect_video(video_file)\n        y=labels.index(df_train[video_files])\n        X_train.append(X)\n        y_train.append(y)\n    return X_train,y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipeline(train_video_files)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It will take too long to execute this so you can try it on your own."},{"metadata":{},"cell_type":"markdown","source":"**<a id=\"5\"></a> <br>**\n## 5- Example of Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/deepfake-detection-challenge/sample_submission.csv\")\nsubmission['label'] = 0.45\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Further More**\n1. try to do this on a better local machine in order to preprocess the whole dataset.\n2. try building a model, for example BiLSTM, CNN, LSTM-CNN."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}