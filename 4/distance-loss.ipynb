{"cells":[{"metadata":{},"cell_type":"markdown","source":"Like [@laevatein](https://www.kaggle.com/laevatein/tweat-the-loss-function-a-bit), I also wanted to give some loss to my model, to make it understand the gap between start/end index.  \n\nSo I was surprised when I found the similar thought in [@laevatein](https://www.kaggle.com/laevatein/tweat-the-loss-function-a-bit) his cool work.\nhttps://www.kaggle.com/laevatein/tweat-the-loss-function-a-bit\n\nI designed a loss function for give 'distance loss' to my model  \n\n### **Distance** btw. prediction's start~end and ground truth's start~end   \n- if prediction's length is too different with ground truth's, then give it panelty(~infinite âˆž)  \n- if both are similar, then near zero panelty\n\n\nIt might be not perfect of course but, I just share my one and looking forward to improving it by someone's feedback.\n\n### Updated\n- updated the one_hot creation codes in 'dist_loss()' by using torch.nn.functional.one_hot ([@Yu Kang](http://https://www.kaggle.com/karlyukang)'s feedback)"},{"metadata":{},"cell_type":"markdown","source":"![](https://user-images.githubusercontent.com/8045508/80869539-22a2a680-8cdc-11ea-85be-d2af6cb7babb.jpeg)"},{"metadata":{},"cell_type":"markdown","source":"lineary increased values is 'linear_func' in the code"},{"metadata":{},"cell_type":"markdown","source":"## Distance Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def dist_between(start_logits, end_logits, device='cpu', max_seq_len=128):\n    \"\"\"get dist btw. pred & ground_truth\"\"\"\n\n    linear_func = torch.tensor(np.linspace(0, 1, max_seq_len, endpoint=False), requires_grad=False)\n    linear_func = linear_func.to(device)\n\n    start_pos = (start_logits*linear_func).sum(axis=1)\n    end_pos = (end_logits*linear_func).sum(axis=1)\n\n    diff = end_pos-start_pos\n\n    return diff.sum(axis=0)/diff.size(0)\n\n\ndef dist_loss(start_logits, end_logits, start_positions, end_positions, device='cpu', max_seq_len=128, scale=1):\n    \"\"\"calculate distance loss between prediction's length & GT's length\n    \n    Input\n    - start_logits ; shape (batch, max_seq_len{128})\n        - logits for start index\n    - end_logits\n        - logits for end index\n    - start_positions ; shape (batch, 1)\n        - start index for GT\n    - end_positions\n        - end index for GT\n    \"\"\"\n    start_logits = torch.nn.Softmax(1)(start_logits) # shape ; (batch, max_seq_len)\n    end_logits = torch.nn.Softmax(1)(end_logits)\n    \n    start_one_hot = torch.nn.functional.one_hot(start_positions, num_classes=max_seq_len).to(device)\n    end_one_hot = torch.nn.functional.one_hot(end_positions, num_classes=max_seq_len).to(device)\n    \n    pred_dist = dist_between(start_logits, end_logits, device, max_seq_len)\n    gt_dist = dist_between(start_one_hot, end_one_hot, device, max_seq_len) # always positive\n    diff = (gt_dist-pred_dist)\n\n    rev_diff_squared = 1-torch.sqrt(diff*diff) # as diff is smaller, make it get closer to the one\n    loss = -torch.log(rev_diff_squared) # by using negative log function, if argument is near zero -> inifinite, near one -> zero\n\n    return loss*scale\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### len(prediction) < len(GT)\n- Prediction ; approximately 1~3\n- GT ; 1~8"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_logits = torch.zeros(10)\nstart_logits[0] = 1\nstart_logits[1] = 8\nstart_logits[2] = 1\n\nend_logits = torch.zeros(10)\nend_logits[2] = 2\nend_logits[3] = 6\nend_logits[4] = 2\n\nstart_pos = torch.tensor(1)\nend_pos = torch.tensor(8)\n\ndist_loss(\n    torch.unsqueeze(start_logits, 0),\n    torch.unsqueeze(end_logits, 0),\n    torch.unsqueeze(start_pos, 0),\n    torch.unsqueeze(end_pos, 0),\n    max_seq_len=10,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### len(prediction) > len(GT)\n- Prediction ; approximately 1~8\n- GT ; 1~1"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_logits = torch.zeros(10)\nstart_logits[0] = 1\nstart_logits[1] = 8\nstart_logits[2] = 1\n\nend_logits = torch.zeros(10)\nend_logits[7] = 1\nend_logits[8] = 8\nend_logits[9] = 1\n\nstart_pos = torch.tensor(1)\nend_pos = torch.tensor(1)\n\ndist_loss(\n    torch.unsqueeze(start_logits, 0),\n    torch.unsqueeze(end_logits, 0),\n    torch.unsqueeze(start_pos, 0),\n    torch.unsqueeze(end_pos, 0),\n    max_seq_len=10,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### len(prediction) ~= len(GT)\n- Prediction ; approximately 1~8\n- GT ; 1~8"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_logits = torch.zeros(10)\nstart_logits[0] = 1\nstart_logits[1] = 8\nstart_logits[2] = 1\n\nend_logits = torch.zeros(10)\nend_logits[7] = 1\nend_logits[8] = 8\nend_logits[9] = 1\n\nstart_pos = torch.tensor(1)\nend_pos = torch.tensor(8)\n\ndist_loss(\n    torch.unsqueeze(start_logits, 0),\n    torch.unsqueeze(end_logits, 0),\n    torch.unsqueeze(start_pos, 0),\n    torch.unsqueeze(end_pos, 0),\n    max_seq_len=10,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Notice\n- distance for the same length decreases, as **max_seq_len** is increasing"},{"metadata":{},"cell_type":"markdown","source":"## Example"},{"metadata":{},"cell_type":"markdown","source":"```\nstart_logits, end_logits = model(token_ids,\n                                 token_type_ids=token_type_ids,\n                                 attention_mask=attention_mask)\n\nstart_loss = torch.nn.CrossEntropyLoss()(start_logits, start_positions)\nend_loss = torch.nn.CrossEntropyLoss()(end_logits, end_positions)\n\nidx_loss = (start_loss+end_loss)\n\ndist_loss = utils.dist_loss(\n    start_logits, end_logits,\n    start_positions, end_positions,\n    device, cfg.MAX_SEQ_LEN) \n\ntotal_loss = idx_loss + dist_loss\n```"},{"metadata":{},"cell_type":"markdown","source":"## Test with...\n- model ; RoBERTa\n- loss ; crossEntropyLoss(with start_logits, end_logits) + distanceLoss\n- max_seq_len ; 128\n- batch size ; 128\n- learning rate ; 9e-5\n- epoch ; 3\n- scheduler ; cosine warmup scheduler\n- early stopping with validation jaccard score & patience=3\n     - checked 4 times in each epochs\n- 5-fold using Stratified K-fold (sklearn) with random seed (293984)\n\n\n## Result\n\n### Not Using Distance Loss ;\n    - 1 fold ; 0.7007\n    - 2 fold ; 0.7088\n    - 3 fold ; 0.7113\n    - 4 fold ; 0.7070\n    - 5 fold ; 0.7041\n    - avg ; 0.7065\n### Using Distance Loss ; \n    - 1 fold ; 0.7061\n    - 2 fold ; 0.7128\n    - 3 fold ; 0.7139\n    - 4 fold ; 0.7043\n    - 5 fold ; 0.7086\n    - avg ; 0.7091"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}