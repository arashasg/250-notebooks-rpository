{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import libraries\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the data into train and test data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsample = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))\nprint(len(test_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Drop the columns id,keyword,location ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_data.target\ntrain_data =train_data.drop(['id','keyword','location','target'],axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Remove all stopwords, retweets, links and hashtags\n* Tokenize the tweets and store them in an array\n* store only stemming words in an array","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text(text):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    text = re.sub(r'\\$\\w*', '', text)\n    # remove old style retweet text \"RT\"\n    text = re.sub(r'^RT[\\s]+', '', text)\n    # remove hyperlinks\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    text_tokens = tokenizer.tokenize(text)\n\n    texts_clean = []\n    for word in text_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            texts_clean.append(stem_word)\n\n\n    return texts_clean\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Get the frequency of each word of both disater and non disaster tweets and store them in freqs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_freqs(texts, ys):\n\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, text in zip(yslist, texts):\n        for word in process_text(text):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs = build_freqs(train_data['text'],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(text, freqs):\n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_text(text)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n    \n    \n    # loop through each word in the list of words\n    for word in word_l:\n        \n        # increment the word count for the positive label 1\n        x[0,1] += freqs.get((word,1.0), 0)\n        \n        # increment the word count for the negative label 0\n        x[0,2] += freqs.get((word,0.0), 0)\n        \n    assert(x.shape == (1, 3))\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.zeros((len(train_data), 3))\nfor i in range(len(train_data)):\n    X[i, :]= extract_features(train_data.text[i], freqs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, accuracy_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(penalty= 'l2' ,random_state= 42 ,max_iter=20,solver='liblinear',class_weight= 'balanced')\nmodel.fit(X[:5500],y[:5500])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X[5500:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(model, X[:5500], y[:5500],labels=[0,1],normalize= 'true')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y[5500:],y_pred)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_model = LogisticRegression(penalty= 'l2' ,random_state= 42 ,max_iter=20,solver='liblinear',class_weight= 'balanced')\nmain_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.zeros((len(test_data), 3))\nfor i in range(len(test_data)):\n    X_test[i, :]= extract_features(test_data.text[i], freqs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"y_test_pred = main_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':sample['id'],'target': y_test_pred})\nsubmission.to_csv('My_submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}